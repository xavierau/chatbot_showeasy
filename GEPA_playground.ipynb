{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import dspy\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "load_dotenv()\n",
    "project_root = Path.cwd()\n",
    "src_path = project_root / \"src\"\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "print(f\"Added {src_path} to Python path\")\n",
    "print(f\"Current working directory: {project_root}\")"
   ],
   "id": "397636721a6c3278",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "api_key=os.getenv('GEMINI_API_KEY')\n",
    "lm = dspy.LM(\"gemini/gemini-2.5-flash-lite\", temperature=1, api_key=api_key)\n",
    "dspy.configure(lm=lm)"
   ],
   "id": "c2393634e482a199",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"pre_guardrails_dataset.csv\")\n",
    "\n",
    "# The is_valid column is already boolean type in the CSV\n",
    "# Check the data type\n",
    "print(f\"is_valid dtype: {df['is_valid'].dtype}\")\n",
    "print(f\"Loaded {len(df)} examples\")\n",
    "df.head()"
   ],
   "id": "c1a3d8530fc55969",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "training_examples = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    # Handle NaN values in conversation_history\n",
    "    conversation_history = row['conversation_history']\n",
    "    if pd.isna(conversation_history) or conversation_history == '':\n",
    "        conversation_history = None\n",
    "\n",
    "    # Handle NaN values in reason\n",
    "    reason = row['reason']\n",
    "    if pd.isna(reason):\n",
    "        reason = \"\"\n",
    "\n",
    "    # Create Example with inputs and expected outputs\n",
    "    example = dspy.Example(\n",
    "        user_message=row['user_input'],\n",
    "        previous_conversation=conversation_history,\n",
    "        page_context=\"\",  # Not provided in dataset\n",
    "        is_valid=row['is_valid'],\n",
    "        violation_type=\"\" if row['is_valid'] else \"unknown\",\n",
    "        user_friendly_message=\"\" if row['is_valid'] else str(reason)\n",
    "    ).with_inputs('user_message', 'previous_conversation', 'page_context')\n",
    "\n",
    "    training_examples.append(example)\n",
    "\n",
    "print(f\"Created {len(training_examples)} DSPy Examples\")\n",
    "print(f\"\\nSample Example:\")\n",
    "print(f\"  Input: {training_examples[0].user_message}\")\n",
    "print(f\"  Expected is_valid: {training_examples[0].is_valid}\")\n",
    "print(f\"  Expected message: {training_examples[0].user_friendly_message}\")"
   ],
   "id": "94d030147053760a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def guardrail_accuracy_metric(example, prediction, trace=None):\n",
    "    \"\"\"\n",
    "    DSPy metric for evaluating input guardrail predictions.\n",
    "\n",
    "    This metric evaluates the core LLM validation (Layer 2) only,\n",
    "    not the pattern-based quick checks (Layer 1).\n",
    "\n",
    "    Args:\n",
    "        example: DSPy.Example containing ground truth 'is_valid' field\n",
    "        prediction: Model prediction with 'is_valid' field\n",
    "        trace: Optional trace for debugging\n",
    "\n",
    "    Returns:\n",
    "        float: 1.0 if prediction matches ground truth, 0.0 otherwise\n",
    "    \"\"\"\n",
    "    ground_truth = example.is_valid\n",
    "\n",
    "    # Handle different prediction formats\n",
    "    if hasattr(prediction, 'is_valid'):\n",
    "        predicted = prediction.is_valid\n",
    "    elif isinstance(prediction, dict) and 'is_valid' in prediction:\n",
    "        predicted = prediction['is_valid']\n",
    "    else:\n",
    "        # If prediction doesn't have is_valid, it's likely a string error response\n",
    "        # Treat as invalid (False)\n",
    "        predicted = False\n",
    "\n",
    "    # Convert string booleans if needed\n",
    "    if isinstance(predicted, str):\n",
    "        predicted = predicted.lower() in ('true', 'yes', '1')\n",
    "\n",
    "    # Both should be boolean values\n",
    "    return float(ground_truth == predicted)"
   ],
   "id": "1ec57a903a3434c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from app.llm.guardrails import PreGuardrails, GuardrailViolation\n",
    "\n",
    "# Initialize the production module (it will auto-load the optimized model)\n",
    "guardrail_module = PreGuardrails()\n",
    "\n",
    "print(\"GuardrailModule created successfully\")\n",
    "print(\"Module uses ChainOfThought with InputGuardrailSignature\")\n",
    "print(\"This module ONLY does Layer 2 LLM validation (not Layer 1 pattern checks)\")"
   ],
   "id": "3719ea92e953af1a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, val_set = train_test_split(\n",
    "    training_examples,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=[ex.is_valid for ex in training_examples]  # Ensure balanced split\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(train_set)} examples\")\n",
    "print(f\"Validation set: {len(val_set)} examples\")\n",
    "print(f\"\\nTraining set distribution:\")\n",
    "print(f\"  Valid inputs: {sum(1 for ex in train_set if ex.is_valid)}\")\n",
    "print(f\"  Invalid inputs: {sum(1 for ex in train_set if not ex.is_valid)}\")\n",
    "print(f\"\\nValidation set distribution:\")\n",
    "print(f\"  Valid inputs: {sum(1 for ex in val_set if ex.is_valid)}\")\n",
    "print(f\"  Invalid inputs: {sum(1 for ex in val_set if not ex.is_valid)}\")"
   ],
   "id": "86051dd599b5230a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from dspy.evaluate import Evaluate\n",
    "\n",
    "evaluator = Evaluate(\n",
    "    devset=val_set,\n",
    "    metric=guardrail_accuracy_metric,\n",
    "    num_threads=2,\n",
    "    display_progress=True,\n",
    "    display_table=5\n",
    ")"
   ],
   "id": "2b489dd63d8451a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "evaluator(guardrail_module)",
   "id": "8337629869834ae2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def feedback_is_valid(gold, pred, trace=None, pred_name=None, pred_trace=None):\n",
    "    print(gold, pred, trace, pred_name, pred_trace)\n",
    "    \"\"\"\n",
    "    Generate feedback.\n",
    "    \"\"\"\n",
    "    score = 1.0 if gold['is_valid'] == pred['is_valid'] else 0.0\n",
    "    if gold == pred:\n",
    "        feedback = f\"You correctly classified the is_valid of the message as `{gold['is_valid']}`. This message is indeed of `{gold['is_valid']}` is_valid.\"\n",
    "    else:\n",
    "        feedback = f\"You incorrectly classified the is_valid of the user message as `{pred['is_valid']}`. The correct is_valid is `{gold['is_valid']}`. Think about how you could have reasoned to get the correct is_valid label.\"\n",
    "\n",
    "    return dspy.Prediction(score=score, feedback=feedback)"
   ],
   "id": "fb926b54b5f53406",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from dspy import GEPA\n",
    "\n",
    "optimizer = GEPA(\n",
    "    metric=feedback_is_valid,\n",
    "    auto=\"light\", # <-- We will use a light budget for this tutorial. However, we typically recommend using auto=\"heavy\" for optimized performance!\n",
    "    num_threads=3,\n",
    "    track_stats=True,\n",
    "    use_merge=False,\n",
    "    reflection_lm=dspy.LM(model=\"gemini/gemini-2.5-pro\", temperature=1.0, max_tokens=32000, api_key=api_key)\n",
    ")"
   ],
   "id": "d6e31556c42303",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "optimized_program = optimizer.compile(\n",
    "    guardrail_module,\n",
    "    trainset=train_set,\n",
    "    valset=val_set,\n",
    ")"
   ],
   "id": "1e17c9fbd3758d4f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for name, pred in optimized_program.named_predictors():\n",
    "    print(\"================================\")\n",
    "    print(f\"Predictor: {name}\")\n",
    "    print(\"================================\")\n",
    "    print(\"Prompt:\")\n",
    "    print(pred.signature.instructions)\n",
    "    print(\"*********************************\")"
   ],
   "id": "af080617235b5f43",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "evaluator(optimized_program)",
   "id": "bb7af4494ea38eab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a80981c55ac239d5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
