{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Optimize ConversationOrchestrator\n",
    "\n",
    "This notebook demonstrates how to train and optimize the refactored ConversationOrchestrator using DSPy.\n",
    "\n",
    "## Training Strategy\n",
    "\n",
    "The ConversationOrchestrator has 3 components:\n",
    "1. **PreGuardrails** - Already optimized (optional: retrain with new data)\n",
    "2. **ReAct Agent** - Main component to optimize with BootstrapFewShot\n",
    "3. **PostGuardrails** - Output validation (optional: optimize if needed)\n",
    "\n",
    "## Optimization Approach\n",
    "\n",
    "We'll use DSPy's BootstrapFewShot teleprompter to optimize the ReAct agent's ConversationSignature.\n",
    "This will create few-shot examples that improve tool selection and response quality."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T06:32:48.168583Z",
     "start_time": "2025-10-08T06:32:48.020479Z"
    }
   },
   "source": [
    "import dspy\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / 'src'))\n",
    "\n",
    "from app.llm.modules import ConversationOrchestrator\n",
    "from app.models import ConversationMessage\n",
    "\n",
    "# Configure DSPy LLM\n",
    "lm = dspy.LM('gemini/gemini-2.5-flash-lite', api_key='AIzaSyB_7NrakdKTUpx6_DtjBgNat1dGWj9G4Ak')\n",
    "dspy.configure(lm=lm)"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'app.llm.tools.AskClarification'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 11\u001B[39m\n\u001B[32m      8\u001B[39m \u001B[38;5;66;03m# Add src to path\u001B[39;00m\n\u001B[32m      9\u001B[39m sys.path.insert(\u001B[32m0\u001B[39m, \u001B[38;5;28mstr\u001B[39m(Path.cwd().parent / \u001B[33m'\u001B[39m\u001B[33msrc\u001B[39m\u001B[33m'\u001B[39m))\n\u001B[32m---> \u001B[39m\u001B[32m11\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mapp\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mllm\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mmodules\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m ConversationOrchestrator\n\u001B[32m     12\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mapp\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mmodels\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m ConversationMessage\n\u001B[32m     14\u001B[39m \u001B[38;5;66;03m# Configure DSPy LLM\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Code/python/showeasy_chatbot/src/app/llm/modules/__init__.py:1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mConversationOrchestrator\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m ConversationOrchestrator\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mQueryGeneration\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m QueryGeneration\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mInputGuardrails\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m InputGuardrails\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Code/python/showeasy_chatbot/src/app/llm/modules/ConversationOrchestrator.py:5\u001B[39m\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01msignatures\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m ConversationSignature\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mmodels\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m ConversationMessage\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mtools\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m      6\u001B[39m     SearchEvent,\n\u001B[32m      7\u001B[39m     MembershipInfo,\n\u001B[32m      8\u001B[39m     TicketInfo,\n\u001B[32m      9\u001B[39m     GeneralHelp,\n\u001B[32m     10\u001B[39m     Thinking,\n\u001B[32m     11\u001B[39m )\n\u001B[32m     12\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mguardrails\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m PreGuardrails, PostGuardrails, GuardrailViolation\n\u001B[32m     15\u001B[39m \u001B[38;5;28;01mclass\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mConversationOrchestrator\u001B[39;00m(dspy.Module):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Code/python/showeasy_chatbot/src/app/llm/tools/__init__.py:5\u001B[39m\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mTicketInfo\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m TicketInfo\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mGeneralHelp\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m GeneralHelp\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mAskClarification\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m AskClarification\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mThinking\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Thinking\n\u001B[32m      8\u001B[39m __all__ = [\n\u001B[32m      9\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mSearchEvent\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     10\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mMembershipInfo\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m     13\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mThinking\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     14\u001B[39m ]\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'app.llm.tools.AskClarification'"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ReAct training data\n",
    "data_path = Path.cwd().parent / \"datasets\" / \"conversation_react_training.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"Loaded {len(df)} training examples\")\n",
    "print(f\"\\nIntent distribution:\")\n",
    "print(df['intent_category'].value_counts())\n",
    "print(f\"\\nLanguage distribution:\")\n",
    "print(df['language'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Examples\n",
    "\n",
    "Convert CSV data into DSPy Examples"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T06:33:41.890722Z",
     "start_time": "2025-10-08T06:33:39.939785Z"
    }
   },
   "source": [
    "def create_dspy_examples(df: pd.DataFrame) -> List[dspy.Example]:\n",
    "    \"\"\"Convert DataFrame to DSPy Examples.\"\"\"\n",
    "    examples = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # Parse conversation history\n",
    "        conv_history = json.loads(row['conversation_history']) if row['conversation_history'] else []\n",
    "        \n",
    "        # Create example\n",
    "        example = dspy.Example(\n",
    "            question=row['user_input'],\n",
    "            previous_conversation=conv_history,\n",
    "            page_context=row['page_context'] if row['page_context'] else \"\",\n",
    "            answer=row['expected_response']\n",
    "        ).with_inputs(\"question\", \"previous_conversation\", \"page_context\")\n",
    "        \n",
    "        examples.append(example)\n",
    "    \n",
    "    return examples\n",
    "\n",
    "# Create examples\n",
    "all_examples = create_dspy_examples(df)\n",
    "\n",
    "# Split train/dev/test (70/15/15)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_dev, test_examples = train_test_split(all_examples, test_size=0.15, random_state=42)\n",
    "train_examples, dev_examples = train_test_split(train_dev, test_size=0.176, random_state=42)  # 0.176 * 0.85 â‰ˆ 0.15\n",
    "\n",
    "print(f\"\\nTrain: {len(train_examples)}\")\n",
    "print(f\"Dev: {len(dev_examples)}\")\n",
    "print(f\"Test: {len(test_examples)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train: 65\n",
      "Dev: 15\n",
      "Test: 15\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T06:33:47.472675Z",
     "start_time": "2025-10-08T06:33:47.463434Z"
    }
   },
   "source": [
    "def conversation_quality_metric(example, prediction, trace=None):\n",
    "    \"\"\"Evaluate conversation quality.\n",
    "    \n",
    "    Checks:\n",
    "    1. Response is not empty\n",
    "    2. Response language matches input language (basic check)\n",
    "    3. Response is helpful (contains substantial content)\n",
    "    \n",
    "    For full evaluation, you'd use semantic similarity or LLM-as-judge.\n",
    "    \"\"\"\n",
    "    # Get prediction response\n",
    "    pred_response = prediction.answer if hasattr(prediction, 'answer') else str(prediction)\n",
    "    \n",
    "    # Check 1: Not empty\n",
    "    if not pred_response or len(pred_response.strip()) < 10:\n",
    "        return 0.0\n",
    "    \n",
    "    # Check 2: Language match (basic heuristic)\n",
    "    # If expected response has Chinese characters, prediction should too\n",
    "    expected = example.answer\n",
    "    has_chinese_expected = any('\\u4e00' <= char <= '\\u9fff' for char in expected)\n",
    "    has_chinese_pred = any('\\u4e00' <= char <= '\\u9fff' for char in pred_response)\n",
    "    \n",
    "    if has_chinese_expected != has_chinese_pred:\n",
    "        return 0.3  # Wrong language, but not completely wrong\n",
    "    \n",
    "    # Check 3: Reasonable length (at least 20% of expected)\n",
    "    if len(pred_response) < len(expected) * 0.2:\n",
    "        return 0.5\n",
    "    \n",
    "    # Basic pass - for full evaluation, use semantic similarity\n",
    "    return 1.0\n",
    "\n",
    "# Test metric\n",
    "test_example = train_examples[0]\n",
    "test_pred = dspy.Prediction(answer=\"This is a test response\")\n",
    "score = conversation_quality_metric(test_example, test_pred)\n",
    "print(f\"Test metric score: {score}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metric score: 0.3\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Performance (Before Optimization)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T06:35:15.225872Z",
     "start_time": "2025-10-08T06:35:15.176554Z"
    }
   },
   "source": [
    "# Initialize unoptimized orchestrator\n",
    "orchestrator = ConversationOrchestrator()\n",
    "\n",
    "# Evaluate on dev set (sample 10 for speed)\n",
    "from dspy.evaluate import Evaluate\n",
    "\n",
    "evaluator = Evaluate(\n",
    "    devset=dev_examples[:10],\n",
    "    metric=conversation_quality_metric,\n",
    "    num_threads=3,\n",
    "    display_progress=True\n",
    ")\n",
    "\n",
    "print(\"\\n=== Baseline Performance ===\")\n",
    "baseline_score = evaluator(orchestrator)\n",
    "print(f\"Baseline Score: {baseline_score:.2%}\")"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ConversationOrchestrator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[16]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Initialize unoptimized orchestrator\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m orchestrator = \u001B[43mConversationOrchestrator\u001B[49m()\n\u001B[32m      4\u001B[39m \u001B[38;5;66;03m# Evaluate on dev set (sample 10 for speed)\u001B[39;00m\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mdspy\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mevaluate\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Evaluate\n",
      "\u001B[31mNameError\u001B[39m: name 'ConversationOrchestrator' is not defined"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize with BootstrapFewShot\n",
    "\n",
    "This will create few-shot examples for the ReAct agent to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T06:33:57.614655Z",
     "start_time": "2025-10-08T06:33:57.571343Z"
    }
   },
   "source": [
    "from dspy.teleprompt import BootstrapFewShot\n",
    "\n",
    "# Configure optimizer\n",
    "optimizer = BootstrapFewShot(\n",
    "    metric=conversation_quality_metric,\n",
    "    max_bootstrapped_demos=8,  # Number of few-shot examples to create\n",
    "    max_labeled_demos=8,  # Use up to 8 labeled examples\n",
    "    max_rounds=1  # Single round of optimization\n",
    ")\n",
    "\n",
    "print(\"\\n=== Optimizing ConversationOrchestrator ===\")\n",
    "print(\"This may take several minutes...\\n\")\n",
    "\n",
    "# Optimize on training set\n",
    "optimized_orchestrator = optimizer.compile(\n",
    "    orchestrator,\n",
    "    trainset=train_examples[:30]  # Use subset for speed (increase for production)\n",
    ")\n",
    "\n",
    "print(\"\\nOptimization complete!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Optimizing ConversationOrchestrator ===\n",
      "This may take several minutes...\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'orchestrator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 16\u001B[39m\n\u001B[32m     12\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mThis may take several minutes...\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     14\u001B[39m \u001B[38;5;66;03m# Optimize on training set\u001B[39;00m\n\u001B[32m     15\u001B[39m optimized_orchestrator = optimizer.compile(\n\u001B[32m---> \u001B[39m\u001B[32m16\u001B[39m     \u001B[43morchestrator\u001B[49m,\n\u001B[32m     17\u001B[39m     trainset=train_examples[:\u001B[32m30\u001B[39m]  \u001B[38;5;66;03m# Use subset for speed (increase for production)\u001B[39;00m\n\u001B[32m     18\u001B[39m )\n\u001B[32m     20\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mOptimization complete!\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mNameError\u001B[39m: name 'orchestrator' is not defined"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Optimized Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T06:34:01.632886Z",
     "start_time": "2025-10-08T06:34:01.612514Z"
    }
   },
   "source": [
    "print(\"\\n=== Optimized Performance ===\")\n",
    "optimized_score = evaluator(optimized_orchestrator)\n",
    "print(f\"Optimized Score: {optimized_score:.2%}\")\n",
    "print(f\"Improvement: {(optimized_score - baseline_score):.2%}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Optimized Performance ===\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'evaluator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[11]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m=== Optimized Performance ===\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m optimized_score = \u001B[43mevaluator\u001B[49m(optimized_orchestrator)\n\u001B[32m      3\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mOptimized Score: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00moptimized_score\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.2%\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m      4\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mImprovement: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m(optimized_score\u001B[38;5;250m \u001B[39m-\u001B[38;5;250m \u001B[39mbaseline_score)\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.2%\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mNameError\u001B[39m: name 'evaluator' is not defined"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Held-Out Test Set"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T06:34:02.439394Z",
     "start_time": "2025-10-08T06:34:02.419612Z"
    }
   },
   "source": [
    "test_evaluator = Evaluate(\n",
    "    devset=test_examples,\n",
    "    metric=conversation_quality_metric,\n",
    "    num_threads=1,\n",
    "    display_progress=True\n",
    ")\n",
    "\n",
    "print(\"\\n=== Test Set Performance ===\")\n",
    "test_score = test_evaluator(optimized_orchestrator)\n",
    "print(f\"Test Score: {test_score:.2%}\")"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Evaluate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m test_evaluator = \u001B[43mEvaluate\u001B[49m(\n\u001B[32m      2\u001B[39m     devset=test_examples,\n\u001B[32m      3\u001B[39m     metric=conversation_quality_metric,\n\u001B[32m      4\u001B[39m     num_threads=\u001B[32m1\u001B[39m,\n\u001B[32m      5\u001B[39m     display_progress=\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m      6\u001B[39m )\n\u001B[32m      8\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m=== Test Set Performance ===\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      9\u001B[39m test_score = test_evaluator(optimized_orchestrator)\n",
      "\u001B[31mNameError\u001B[39m: name 'Evaluate' is not defined"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Optimized Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T06:34:07.938556Z",
     "start_time": "2025-10-08T06:34:07.872734Z"
    }
   },
   "source": [
    "# Create output directory\n",
    "output_dir = Path.cwd().parent / \"src\" / \"app\" / \"optimized\" / \"ConversationOrchestrator\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save optimized model\n",
    "output_path = output_dir / \"optimized_model.json\"\n",
    "optimized_orchestrator.save(str(output_path))\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    \"baseline_score\": float(baseline_score),\n",
    "    \"optimized_score\": float(optimized_score),\n",
    "    \"test_score\": float(test_score),\n",
    "    \"training_examples\": len(train_examples),\n",
    "    \"optimization_date\": pd.Timestamp.now().isoformat(),\n",
    "    \"max_bootstrapped_demos\": 8,\n",
    "    \"notes\": \"Optimized with BootstrapFewShot on ReAct agent\"\n",
    "}\n",
    "\n",
    "with open(output_dir / \"metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\nOptimized model saved to: {output_path}\")\n",
    "print(f\"Metadata saved to: {output_dir / 'metadata.json'}\")"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimized_orchestrator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[13]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      5\u001B[39m \u001B[38;5;66;03m# Save optimized model\u001B[39;00m\n\u001B[32m      6\u001B[39m output_path = output_dir / \u001B[33m\"\u001B[39m\u001B[33moptimized_model.json\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m \u001B[43moptimized_orchestrator\u001B[49m.save(\u001B[38;5;28mstr\u001B[39m(output_path))\n\u001B[32m      9\u001B[39m \u001B[38;5;66;03m# Save metadata\u001B[39;00m\n\u001B[32m     10\u001B[39m metadata = {\n\u001B[32m     11\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mbaseline_score\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28mfloat\u001B[39m(baseline_score),\n\u001B[32m     12\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33moptimized_score\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28mfloat\u001B[39m(optimized_score),\n\u001B[32m   (...)\u001B[39m\u001B[32m     17\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mnotes\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mOptimized with BootstrapFewShot on ReAct agent\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     18\u001B[39m }\n",
      "\u001B[31mNameError\u001B[39m: name 'optimized_orchestrator' is not defined"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Testing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T06:34:09.093198Z",
     "start_time": "2025-10-08T06:34:09.055292Z"
    }
   },
   "source": [
    "def test_orchestrator(user_input: str, page_context: str = \"\"):\n",
    "    \"\"\"Test orchestrator with a query.\"\"\"\n",
    "    print(f\"\\nUser: {user_input}\")\n",
    "    print(f\"Context: {page_context or 'None'}\")\n",
    "    print(\"\\n--- Processing ---\\n\")\n",
    "    \n",
    "    response = optimized_orchestrator(\n",
    "        user_message=user_input,\n",
    "        previous_conversation=[],\n",
    "        page_context=page_context\n",
    "    )\n",
    "    \n",
    "    print(f\"Agent: {response}\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Test examples\n",
    "test_orchestrator(\"Find rock concerts in Los Angeles\")\n",
    "test_orchestrator(\"How much is membership?\")\n",
    "test_orchestrator(\"æ‰¾éŸ³æ¨‚æœƒ\")\n",
    "test_orchestrator(\"I want jazz shows this weekend and what's the refund policy?\")  # Multi-intent"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User: Find rock concerts in Los Angeles\n",
      "Context: None\n",
      "\n",
      "--- Processing ---\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'optimized_orchestrator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[14]\u001B[39m\u001B[32m, line 17\u001B[39m\n\u001B[32m     14\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m + \u001B[33m\"\u001B[39m\u001B[33m=\u001B[39m\u001B[33m\"\u001B[39m*\u001B[32m80\u001B[39m)\n\u001B[32m     16\u001B[39m \u001B[38;5;66;03m# Test examples\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m17\u001B[39m \u001B[43mtest_orchestrator\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mFind rock concerts in Los Angeles\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     18\u001B[39m test_orchestrator(\u001B[33m\"\u001B[39m\u001B[33mHow much is membership?\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     19\u001B[39m test_orchestrator(\u001B[33m\"\u001B[39m\u001B[33mæ‰¾éŸ³æ¨‚æœƒ\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[14]\u001B[39m\u001B[32m, line 7\u001B[39m, in \u001B[36mtest_orchestrator\u001B[39m\u001B[34m(user_input, page_context)\u001B[39m\n\u001B[32m      4\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mContext: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpage_context\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01mor\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[33m'\u001B[39m\u001B[33mNone\u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m      5\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m--- Processing ---\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m response = \u001B[43moptimized_orchestrator\u001B[49m(\n\u001B[32m      8\u001B[39m     user_message=user_input,\n\u001B[32m      9\u001B[39m     previous_conversation=[],\n\u001B[32m     10\u001B[39m     page_context=page_context\n\u001B[32m     11\u001B[39m )\n\u001B[32m     13\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mAgent: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     14\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m + \u001B[33m\"\u001B[39m\u001B[33m=\u001B[39m\u001B[33m\"\u001B[39m*\u001B[32m80\u001B[39m)\n",
      "\u001B[31mNameError\u001B[39m: name 'optimized_orchestrator' is not defined"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: Tool Usage Patterns"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T06:34:11.829187Z",
     "start_time": "2025-10-08T06:34:11.827064Z"
    }
   },
   "source": [
    "# Analyze which tools are being called\n",
    "print(\"\\n=== Tool Usage Analysis ===\")\n",
    "print(\"Run this after testing several queries to see tool call patterns\")\n",
    "print(\"You can inspect dspy.settings.trace to see tool calls\")\n",
    "\n",
    "# Note: Full trace analysis would require custom logging in the orchestrator\n",
    "# For production, add logging to track:\n",
    "# - Which tools are called for each intent\n",
    "# - Tool call success rates\n",
    "# - Average response latency per tool"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Tool Usage Analysis ===\n",
      "Run this after testing several queries to see tool call patterns\n",
      "You can inspect dspy.settings.trace to see tool calls\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Load optimized model in production**:\n",
    "   ```python\n",
    "   orchestrator = ConversationOrchestrator()\n",
    "   orchestrator.load('path/to/optimized_model.json')\n",
    "   ```\n",
    "\n",
    "2. **Collect real user data** and retrain periodically\n",
    "\n",
    "3. **Optimize guardrails** separately if needed:\n",
    "   - PreGuardrails: Use existing dataset\n",
    "   - PostGuardrails: Use generated dataset from notebook 02\n",
    "\n",
    "4. **Advanced optimization**:\n",
    "   - Use MIPROv2 for more sophisticated optimization\n",
    "   - Create LLM-as-judge metric for semantic similarity\n",
    "   - Add A/B testing framework\n",
    "\n",
    "5. **Monitor in production**:\n",
    "   - Track tool usage rates\n",
    "   - Monitor response quality\n",
    "   - Collect edge cases for retraining"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
