{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-Stage Optimization: BootstrapFewShot → GEPA\n",
    "\n",
    "This notebook demonstrates a two-stage optimization approach for the ConversationOrchestrator:\n",
    "\n",
    "## Stage 1: BootstrapFewShot\n",
    "- Generates high-quality few-shot examples from training data\n",
    "- Fast iteration (~15-30 min)\n",
    "- Establishes baseline with good demonstrations\n",
    "\n",
    "## Stage 2: GEPA (Genetic-Evolutionary Prompt Adaptation)\n",
    "- Takes Stage 1 output as starting point\n",
    "- Uses LLM reflection to evolve prompt instructions\n",
    "- Optimizes tool selection patterns\n",
    "- More sophisticated, slower (~1-2 hours)\n",
    "\n",
    "## Why Two Stages?\n",
    "1. **BootstrapFewShot**: Finds good examples of correct behavior\n",
    "2. **GEPA**: Refines the instructions that guide the ReAct agent\n",
    "3. **Combined**: Best of both - good examples AND evolved prompts\n",
    "\n",
    "## Models Used\n",
    "- **Advanced Model**: `gemini/gemini-2.5-pro` - For reflection and optimization\n",
    "- **Production Model**: `gemini/gemini-2.5-flash-lite` - For running the orchestrator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import dspy\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from typing import List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / 'src'))\n",
    "\n",
    "from app.llm.modules import ConversationOrchestrator\n",
    "from app.models import ConversationMessage"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure DSPy with Gemini Models"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "# Configure production model (for running orchestrator)\n",
    "production_lm = dspy.LM('gemini/gemini-2.0-flash-lite', api_key=\"AIzaSyB_7NrakdKTUpx6_DtjBgNat1dGWj9G4Ak\")\n",
    "dspy.configure(lm=production_lm)\n",
    "\n",
    "# Configure advanced model (for optimization/reflection)\n",
    "advanced_lm = dspy.LM('gemini/gemini-2.5-pro', api_key=\"AIzaSyB_7NrakdKTUpx6_DtjBgNat1dGWj9G4Ak\")\n",
    "\n",
    "print(\"✓ Configured models:\")\n",
    "print(f\"  Production: gemini/gemini-2.0-flash-lite\")\n",
    "print(f\"  Advanced: gemini/gemini-2.5-pro\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def load_training_data(csv_path: str) -> Tuple[List[dspy.Example], List[dspy.Example]]:\n",
    "    \"\"\"Load and split training data.\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    examples = []\n",
    "    for _, row in df.iterrows():\n",
    "        conv_history = json.loads(row['conversation_history']) if row['conversation_history'] else []\n",
    "        \n",
    "        example = dspy.Example(\n",
    "            question=row['user_input'],\n",
    "            previous_conversation=conv_history,\n",
    "            page_context=row['page_context'] if row['page_context'] else \"\",\n",
    "            answer=row['expected_response']\n",
    "        ).with_inputs(\"question\", \"previous_conversation\", \"page_context\")\n",
    "        \n",
    "        examples.append(example)\n",
    "    \n",
    "    # Split 80/20 train/dev\n",
    "    split_idx = int(len(examples) * 0.8)\n",
    "    return examples[:split_idx], examples[split_idx:]\n",
    "\n",
    "# Load data\n",
    "data_path = Path.cwd().parent / \"datasets\" / \"conversation_react_training.csv\"\n",
    "\n",
    "if data_path.exists():\n",
    "    trainset, devset = load_training_data(str(data_path))\n",
    "    print(f\"✓ Loaded training data:\")\n",
    "    print(f\"  Train: {len(trainset)} examples\")\n",
    "    print(f\"  Dev: {len(devset)} examples\")\n",
    "else:\n",
    "    print(f\"⚠ Dataset not found at {data_path}\")\n",
    "    print(\"  Run notebook 01_generate_react_training_data.ipynb first\")\n",
    "    trainset, devset = [], []"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def conversation_quality_metric(example, prediction, trace=None):\n",
    "    \"\"\"Multi-faceted metric for conversation quality.\n",
    "    \n",
    "    Returns:\n",
    "        float or tuple: Score 0.0-1.0, optionally with textual feedback for GEPA\n",
    "    \"\"\"\n",
    "    score = 0.0\n",
    "    feedback = []\n",
    "    \n",
    "    # Get prediction response\n",
    "    pred_response = prediction.answer if hasattr(prediction, 'answer') else str(prediction)\n",
    "    \n",
    "    # 1. Response exists and is substantial (30%)\n",
    "    if pred_response and len(pred_response.strip()) > 20:\n",
    "        score += 0.3\n",
    "    else:\n",
    "        feedback.append(\"Response too short or empty\")\n",
    "    \n",
    "    # 2. Language consistency (30%)\n",
    "    expected = example.answer\n",
    "    has_chinese_expected = any('\\u4e00' <= char <= '\\u9fff' for char in expected)\n",
    "    has_chinese_pred = any('\\u4e00' <= char <= '\\u9fff' for char in pred_response)\n",
    "    \n",
    "    if has_chinese_expected == has_chinese_pred:\n",
    "        score += 0.3\n",
    "    else:\n",
    "        lang_expected = \"Chinese\" if has_chinese_expected else \"English\"\n",
    "        feedback.append(f\"Language mismatch: expected {lang_expected}\")\n",
    "    \n",
    "    # 3. Reasonable length relative to expected (20%)\n",
    "    if len(pred_response) >= len(expected) * 0.3:\n",
    "        score += 0.2\n",
    "    else:\n",
    "        feedback.append(\"Response significantly shorter than expected\")\n",
    "    \n",
    "    # 4. Basic content check (20%)\n",
    "    # Check if response seems relevant\n",
    "    question_lower = example.question.lower()\n",
    "    response_lower = pred_response.lower()\n",
    "    \n",
    "    relevance = False\n",
    "    if \"membership\" in question_lower and (\"membership\" in response_lower or \"member\" in response_lower):\n",
    "        relevance = True\n",
    "    elif \"ticket\" in question_lower and \"ticket\" in response_lower:\n",
    "        relevance = True\n",
    "    elif \"event\" in question_lower or \"concert\" in question_lower:\n",
    "        relevance = True\n",
    "    elif \"音樂\" in example.question or \"活動\" in example.question:\n",
    "        relevance = True\n",
    "    else:\n",
    "        relevance = len(pred_response) > 30  # At least trying to help\n",
    "    \n",
    "    if relevance:\n",
    "        score += 0.2\n",
    "    else:\n",
    "        feedback.append(\"Response may not be relevant to query\")\n",
    "    \n",
    "    # Return score with feedback for GEPA (feedback helps reflection)\n",
    "    if feedback:\n",
    "        return (score, \" | \".join(feedback))\n",
    "    return score\n",
    "\n",
    "# Test metric\n",
    "if trainset:\n",
    "    test_example = trainset[0]\n",
    "    test_pred = dspy.Prediction(answer=\"This is a test response with adequate length.\")\n",
    "    test_score = conversation_quality_metric(test_example, test_pred)\n",
    "    print(f\"✓ Metric test score: {test_score if isinstance(test_score, float) else test_score[0]:.2f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from dspy.evaluate import Evaluate\n",
    "\n",
    "# Initialize unoptimized orchestrator\n",
    "orchestrator = ConversationOrchestrator()\n",
    "\n",
    "# Evaluate on dev set (sample for speed)\n",
    "evaluator = Evaluate(\n",
    "    devset=devset[:10] if len(devset) > 10 else devset,\n",
    "    metric=conversation_quality_metric,\n",
    "    num_threads=1,\n",
    "    display_progress=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BASELINE EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "baseline_score = evaluator(orchestrator)\n",
    "print(f\"\\n✓ Baseline Score: {baseline_score:.2%}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STAGE 1: BootstrapFewShot Optimization\n",
    "\n",
    "Generate high-quality few-shot examples by running the orchestrator multiple times and collecting successful trajectories."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from dspy.teleprompt import BootstrapFewShot\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAGE 1: BootstrapFewShot Optimization\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Configure BootstrapFewShot\n",
    "bootstrap_optimizer = BootstrapFewShot(\n",
    "    metric=conversation_quality_metric,\n",
    "    max_bootstrapped_demos=8,  # Generate up to 8 examples\n",
    "    max_labeled_demos=4,       # Use up to 4 labeled examples\n",
    "    max_rounds=2,              # Bootstrap rounds\n",
    "    max_errors=10              # Allow some failures during bootstrapping\n",
    ")\n",
    "\n",
    "print(\"\\nOptimizer configuration:\")\n",
    "print(f\"  max_bootstrapped_demos: 8\")\n",
    "print(f\"  max_labeled_demos: 4\")\n",
    "print(f\"  max_rounds: 2\")\n",
    "print(\"\\nStarting optimization (this may take 15-30 minutes)...\\n\")\n",
    "\n",
    "# Compile with training set\n",
    "stage1_optimized = bootstrap_optimizer.compile(\n",
    "    student=orchestrator,\n",
    "    trainset=trainset[:30] if len(trainset) > 30 else trainset  # Use subset for speed\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Stage 1 optimization complete!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Stage 1 Results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\\nEvaluating Stage 1 (BootstrapFewShot) results...\\n\")\n",
    "\n",
    "stage1_score = evaluator(stage1_optimized)\n",
    "stage1_improvement = stage1_score - baseline_score\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"STAGE 1 RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Baseline Score:    {baseline_score:.2%}\")\n",
    "print(f\"Stage 1 Score:     {stage1_score:.2%}\")\n",
    "print(f\"Improvement:       +{stage1_improvement:.2%}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save Stage 1 checkpoint\n",
    "checkpoint_path = Path.cwd().parent / \"src\" / \"app\" / \"optimized\" / \"ConversationOrchestrator\"\n",
    "checkpoint_path.mkdir(parents=True, exist_ok=True)\n",
    "stage1_optimized.save(str(checkpoint_path / \"stage1_bootstrap.json\"))\n",
    "print(f\"\\n✓ Stage 1 checkpoint saved to: {checkpoint_path / 'stage1_bootstrap.json'}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Stage 1 Few-Shot Examples\n",
    "\n",
    "Let's look at the examples that BootstrapFewShot generated"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\\nGenerated Few-Shot Examples from Stage 1:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Access demos from the agent's signature\n",
    "if hasattr(stage1_optimized, 'agent') and hasattr(stage1_optimized.agent, 'demos'):\n",
    "    demos = stage1_optimized.agent.demos\n",
    "    print(f\"Total demos: {len(demos)}\\n\")\n",
    "    \n",
    "    for i, demo in enumerate(demos[:3]):  # Show first 3\n",
    "        print(f\"\\nDemo {i+1}:\")\n",
    "        print(f\"  Question: {demo.question}\")\n",
    "        print(f\"  Answer: {demo.answer[:150]}...\" if len(demo.answer) > 150 else f\"  Answer: {demo.answer}\")\n",
    "        print(\"-\" * 80)\n",
    "else:\n",
    "    print(\"No demos found in optimized model structure\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STAGE 2: GEPA Optimization\n",
    "\n",
    "Now we use GEPA to evolve the prompt instructions through reflection, starting from the Stage 1 output."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from dspy.teleprompt import GEPA\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAGE 2: GEPA Optimization\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Configure GEPA optimizer\n",
    "gepa_optimizer = dspy.GEPA(\n",
    "    metric=conversation_quality_metric,\n",
    "    reflection_lm=advanced_lm,  # Use Gemini 2.5 Pro for reflection\n",
    "    auto=\"medium\",  # Can be: \"light\", \"medium\", \"heavy\"\n",
    "    num_threads=4,\n",
    "    track_stats=True,\n",
    "    component_selector=\"all\"  # Optimize all components together\n",
    ")\n",
    "\n",
    "print(\"\\nOptimizer configuration:\")\n",
    "print(f\"  reflection_lm: gemini/gemini-2.5-pro\")\n",
    "print(f\"  auto: medium\")\n",
    "print(f\"  num_threads: 4\")\n",
    "print(f\"  component_selector: all\")\n",
    "print(\"\\nStarting GEPA optimization (this may take 1-2 hours)...\\n\")\n",
    "\n",
    "# Compile using Stage 1 output as starting point\n",
    "stage2_optimized = gepa_optimizer.compile(\n",
    "    student=stage1_optimized,  # IMPORTANT: Start from Stage 1 result\n",
    "    trainset=trainset[:30] if len(trainset) > 30 else trainset,\n",
    "    valset=devset,\n",
    "    num_iterations=5  # GEPA iterations\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Stage 2 optimization complete!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Stage 2 Results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\\nEvaluating Stage 2 (GEPA) results...\\n\")\n",
    "\n",
    "stage2_score = evaluator(stage2_optimized)\n",
    "stage2_improvement = stage2_score - stage1_score\n",
    "total_improvement = stage2_score - baseline_score\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"FINAL RESULTS - TWO-STAGE OPTIMIZATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Baseline Score:           {baseline_score:.2%}\")\n",
    "print(f\"Stage 1 (Bootstrap):      {stage1_score:.2%}  (+{(stage1_score - baseline_score):.2%})\")\n",
    "print(f\"Stage 2 (GEPA):           {stage2_score:.2%}  (+{stage2_improvement:.2%})\")\n",
    "print(f\"\\nTotal Improvement:        +{total_improvement:.2%}\")\n",
    "print(\"=\"*80)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create comparison DataFrame\n",
    "results = pd.DataFrame({\n",
    "    'Stage': ['Baseline', 'BootstrapFewShot', 'GEPA'],\n",
    "    'Score': [baseline_score, stage1_score, stage2_score],\n",
    "    'Improvement': [0, stage1_score - baseline_score, total_improvement]\n",
    "})\n",
    "\n",
    "print(\"\\n\", results)\n",
    "\n",
    "# Plot results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Score progression\n",
    "ax1.bar(results['Stage'], results['Score'], color=['#ff6b6b', '#4ecdc4', '#45b7d1'])\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_title('Two-Stage Optimization Results')\n",
    "ax1.set_ylim(0, 1.0)\n",
    "for i, v in enumerate(results['Score']):\n",
    "    ax1.text(i, v + 0.02, f'{v:.2%}', ha='center', fontweight='bold')\n",
    "\n",
    "# Improvement bars\n",
    "ax2.bar(results['Stage'], results['Improvement'], color=['#ff6b6b', '#4ecdc4', '#45b7d1'])\n",
    "ax2.set_ylabel('Improvement over Baseline')\n",
    "ax2.set_title('Improvement by Stage')\n",
    "for i, v in enumerate(results['Improvement']):\n",
    "    if v > 0:\n",
    "        ax2.text(i, v + 0.01, f'+{v:.2%}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect GEPA-Evolved Instructions\n",
    "\n",
    "Let's see how GEPA evolved the prompt instructions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GEPA-EVOLVED INSTRUCTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if hasattr(stage2_optimized, 'agent') and hasattr(stage2_optimized.agent, 'signature'):\n",
    "    evolved_instructions = stage2_optimized.agent.signature.instructions\n",
    "    print(evolved_instructions)\n",
    "else:\n",
    "    print(\"Could not access evolved instructions\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ORIGINAL INSTRUCTIONS (for comparison)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if hasattr(orchestrator, 'agent') and hasattr(orchestrator.agent, 'signature'):\n",
    "    original_instructions = orchestrator.agent.signature.instructions\n",
    "    print(original_instructions)\n",
    "else:\n",
    "    print(\"Could not access original instructions\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Testing\n",
    "\n",
    "Compare all three versions side-by-side"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def test_all_stages(query: str, page_context: str = \"\"):\n",
    "    \"\"\"Test query across all optimization stages.\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    if page_context:\n",
    "        print(f\"Context: {page_context}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    try:\n",
    "        print(\"BASELINE:\")\n",
    "        baseline_response = orchestrator(query, [], page_context)\n",
    "        print(baseline_response)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "    \n",
    "    try:\n",
    "        print(\"STAGE 1 (BootstrapFewShot):\")\n",
    "        stage1_response = stage1_optimized(query, [], page_context)\n",
    "        print(stage1_response)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "    \n",
    "    try:\n",
    "        print(\"STAGE 2 (GEPA):\")\n",
    "        stage2_response = stage2_optimized(query, [], page_context)\n",
    "        print(stage2_response)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Test various query types\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"# INTERACTIVE TESTING\")\n",
    "print(\"#\"*80)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Test 1: Simple event search\n",
    "test_all_stages(\"Find rock concerts in Los Angeles\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Test 2: Membership inquiry\n",
    "test_all_stages(\"How much is premium membership?\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Test 3: Chinese query\n",
    "test_all_stages(\"找音樂會\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Test 4: Multi-intent query\n",
    "test_all_stages(\"I want jazz concerts this weekend and what's the refund policy?\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Test 5: Vague query\n",
    "test_all_stages(\"events\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Final Optimized Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save final optimized model\n",
    "output_dir = Path.cwd().parent / \"src\" / \"app\" / \"optimized\" / \"ConversationOrchestrator\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "final_model_path = output_dir / \"two_stage_optimized.json\"\n",
    "stage2_optimized.save(str(final_model_path))\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'optimization_method': 'two_stage',\n",
    "    'stage1': {\n",
    "        'optimizer': 'BootstrapFewShot',\n",
    "        'max_bootstrapped_demos': 8,\n",
    "        'max_labeled_demos': 4,\n",
    "        'max_rounds': 2,\n",
    "        'score': float(stage1_score)\n",
    "    },\n",
    "    'stage2': {\n",
    "        'optimizer': 'GEPA',\n",
    "        'reflection_lm': 'gemini/gemini-2.5-pro',\n",
    "        'auto': 'medium',\n",
    "        'num_iterations': 5,\n",
    "        'score': float(stage2_score)\n",
    "    },\n",
    "    'baseline_score': float(baseline_score),\n",
    "    'final_score': float(stage2_score),\n",
    "    'total_improvement': float(total_improvement),\n",
    "    'training_examples': len(trainset),\n",
    "    'dev_examples': len(devset),\n",
    "    'optimization_date': pd.Timestamp.now().isoformat(),\n",
    "    'models': {\n",
    "        'production': 'gemini/gemini-2.0-flash-lite',\n",
    "        'advanced': 'gemini/gemini-2.5-pro'\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_path = output_dir / \"two_stage_metadata.json\"\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ OPTIMIZATION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nFinal model saved to:\")\n",
    "print(f\"  {final_model_path}\")\n",
    "print(f\"\\nMetadata saved to:\")\n",
    "print(f\"  {metadata_path}\")\n",
    "print(f\"\\nFinal Score: {stage2_score:.2%}\")\n",
    "print(f\"Total Improvement: +{total_improvement:.2%}\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage in Production\n",
    "\n",
    "To use the optimized model:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Example: Load and use optimized model\n",
    "from app.llm.modules import ConversationOrchestrator\n",
    "\n",
    "# Load optimized model\n",
    "optimized = ConversationOrchestrator()\n",
    "optimized.load('src/app/optimized/ConversationOrchestrator/two_stage_optimized.json')\n",
    "\n",
    "# Use in production\n",
    "response = optimized(\n",
    "    user_message=\"Find jazz concerts\",\n",
    "    previous_conversation=[],\n",
    "    page_context=\"\"\n",
    ")\n",
    "print(response)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Achieved\n",
    "\n",
    "1. **Stage 1 (BootstrapFewShot)**:\n",
    "   - Generated high-quality few-shot examples\n",
    "   - Fast optimization (~15-30 minutes)\n",
    "   - Established strong baseline\n",
    "\n",
    "2. **Stage 2 (GEPA)**:\n",
    "   - Evolved prompt instructions through LLM reflection\n",
    "   - Optimized tool selection patterns\n",
    "   - Combined with Stage 1 examples for best results\n",
    "\n",
    "3. **Final Model**:\n",
    "   - Optimized few-shot demonstrations\n",
    "   - Evolved prompt instructions\n",
    "   - Improved tool usage patterns\n",
    "   - Better multilingual consistency\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Deploy**: Use the optimized model in production\n",
    "2. **Monitor**: Track performance on real user queries\n",
    "3. **Iterate**: Collect edge cases and retrain periodically\n",
    "4. **A/B Test**: Compare optimized vs baseline in production"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
